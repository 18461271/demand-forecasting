{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy.stats import describe\n#from keras.utils import np_utils\n#%matplotlib inline\nimport tensorflow as tf\nimport sys\nfrom keras.backend.tensorflow_backend import set_session\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Concatenate, concatenate, Input, Reshape, LSTM, TimeDistributed\nfrom keras.layers import LSTM, Dense, Conv1D, Input, Dropout, AvgPool1D, Reshape, Concatenate\nfrom keras.layers.embeddings import Embedding\n#import seaborn as sns\nimport warnings\nfrom keras import models, layers\n # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import MinMaxScaler\n\nwarnings.filterwarnings(\"ignore\")\nprint(\"python {}\".format(sys.version))\nprint(\"keras version {}\".format(keras.__version__))\nprint(\"tensorflow version {}\".format(tf.__version__))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92b3f3d17ee8ade5b764c4bdc083b481be59fdbd"
      },
      "cell_type": "code",
      "source": "# Loading the data\ndf_train  = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])\ndf_test = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('../input/demand-forecasting-kernels-only/sample_submission.csv')\nprint('Train shape:{}, Test shape:{}'.format(df_train.shape, df_test.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07d9a54d697e6aad712535993f17cd4887c8aa3e"
      },
      "cell_type": "code",
      "source": "# data processing\nimport os, sys, time, math, json, random\n#import tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\ndef set_seed(sd=123): #define seed for consistent output\n    from numpy.random import seed\n    from tensorflow import set_random_seed\n    import random as rn\n    ## numpy random seed\n    seed(sd)\n    ## core python's random number\n    rn.seed(sd)\n    ## tensor flow's random number\n    set_random_seed(sd)\n#set_seed(sd=123)\nimport sys\nfrom itertools import product, starmap\n\ndef storeitems():\n    return product(range(1,51), range(1,11))\n\n\ndef storeitems_column_names():\n    return list(starmap(lambda i,s: f'item_{i}_store_{s}_sales', storeitems()))\n\n\ndef sales_by_storeitem(df):\n    ret = pd.DataFrame(index=df.index.unique())\n    for i, s in storeitems():\n        #print(i,s)\n        #sys.exit()\n\n        ret[f'item_{i}_store_{s}_sales'] = df[(df['item'] == i) & (df['store'] == s)]['sales'].values\n        #print( ret)\n    return ret\n\ndef shift_series(series, days):\n    return series.transform(lambda x: x.shift(days))\n\n\ndef shift_series_in_df(df, series_names=[], days_delta=90):\n    ret = pd.DataFrame(index=df.index.copy())\n    str_sgn = 'future' if np.sign(days_delta) < 0 else 'past'\n    for sn in series_names:\n        ret[f'{sn}_{str_sgn}_{np.abs(days_delta)}'] = shift_series(df[sn], days_delta)\n    return ret\n\n\ndef stack_shifted_sales(df, days_deltas=[1, 90, 360]):\n    names = storeitems_column_names()\n    dfs = [df.copy()]\n    #print(dfs)\n    #sys.exit()\n    for delta in days_deltas:\n        shifted = shift_series_in_df(df, series_names=names, days_delta=delta)\n        #print(shifted.shape)\n        #sys.exit()\n        dfs.append(shifted)\n    return pd.concat(dfs, axis=1, sort=False, copy=False)\n\ndef generate_data(df_train, df_test ):\n    df_train.index = pd.to_datetime(df_train['date'])\n    df_train.drop('date', axis=1, inplace=True)\n    df_train = sales_by_storeitem(df_train)\n\n    df_test.index = pd.to_datetime(df_test['date'])\n    df_test.drop('date', axis=1, inplace=True)\n    df_test['sales'] = np.zeros(df_test.shape[0])\n    df_test = sales_by_storeitem(df_test)\n\n    col_names = list(zip(df_test.columns, df_train.columns))  #('item_1_store_1_sales', 'item_1_store_1_sales')\n    for cn in col_names:\n        assert cn[0] == cn[1]\n\n    df_test['is_test'] = np.repeat(True, df_test.shape[0])\n    df_train['is_test'] = np.repeat(False, df_train.shape[0])\n    df_total = pd.concat([df_train, df_test])\n    weekday_df = pd.get_dummies(df_total.index.weekday, prefix='weekday')\n    weekday_df.index = df_total.index\n    month_df = pd.get_dummies(df_total.index.month, prefix='month')\n    month_df.index =  df_total.index\n    df_total = pd.concat([weekday_df, month_df, df_total], axis=1)\n    assert df_total.isna().any().any() == False\n    df_total = stack_shifted_sales(df_total, days_deltas=[1])\n    df_total.dropna(inplace=True)\n    sales_cols = [col for col in df_total.columns if '_sales' in col and '_sales_' not in col]\n    stacked_sales_cols = [col for col in df_total.columns if '_sales_' in col]\n    other_cols = [col for col in df_total.columns if col not in set(sales_cols) and col not in set(stacked_sales_cols)]\n\n    sales_cols = sorted(sales_cols)\n    stacked_sales_cols = sorted(stacked_sales_cols)\n\n    new_cols = other_cols + stacked_sales_cols + sales_cols\n    df_total = df_total.reindex(columns=new_cols)\n    assert df_total.isna().any().any() == False\n    scaler = MinMaxScaler(feature_range=(0,1))\n    cols_to_scale = [col for col in df_total.columns if 'weekday' not in col and 'month' not in col and \"is_test\" not in col]\n    scaled_cols = scaler.fit_transform(df_total[cols_to_scale])\n    df_total[cols_to_scale] = scaled_cols\n    df_train = df_total[df_total['is_test'] == False].drop('is_test', axis=1)\n    df_test = df_total[df_total['is_test'] == True].drop('is_test', axis=1)\n\n    X_cols_stacked = [col for col in df_train.columns if '_past_' in col]\n    X_cols_caldata = [col for col in df_train.columns if 'weekday_' in col or 'month_' in col or 'year' in col]\n    X_cols = X_cols_stacked + X_cols_caldata\n\n    X = df_train[X_cols]\n    y_cols = [col for col in df_train.columns if col not in X_cols]\n    y = df_train[y_cols]\n    X_test = df_test[X_cols]\n    return X, y, X_test, scaler, df_test, cols_to_scale, y_cols",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c00117f5c63c2a3d9ec3b9d827da0d7c9d6060ef"
      },
      "cell_type": "code",
      "source": "def define_model2(time_steps,\n                    hidden_neurons = 50,\n                    batch_size=None,\n                    feature_dim =20,\n                    stateful=False):\n    output_neurons = 1\n\n    inputs = Input( batch_shape=(batch_size,time_steps, feature_dim) )  #Input(shape=(X_train.shape[1], X_train.shape[2]))\n\n    top_lstm  = LSTM( hidden_neurons,activation='sigmoid',kernel_initializer='normal', dropout=0.25, return_sequences=True, stateful=stateful, name=\"lstm\")(inputs)\n    top_dense = TimeDistributed( Dense(50, activation='relu'))(top_lstm)\n    top_dropout = TimeDistributed(Dropout(50))(top_dense)\n    # bottom pipeline\n    bottom_dense = TimeDistributed(Dense(50))(inputs)\n    bottom_conv1 = Conv1D(\n                            50,\n                            kernel_size=1,\n                            input_shape=(batch_size, time_steps, feature_dim)#input_shape=(X_train.shape[1], X_train.shape[2])\n                        )(bottom_dense)\n    bottom_conv2 = Conv1D(\n                    100,\n                    kernel_size=50,\n                    padding='same',\n                    activation='relu'\n                )(bottom_conv1)\n    bottom_conv3 = Conv1D(\n                    50,\n                    kernel_size=10,\n                    padding='same',\n                    activation='relu'\n                )(bottom_conv2)\n    bottom_pooling = AvgPool1D(\n                    pool_size=60,\n                    padding='same'\n                )(bottom_conv3)\n    bottom_reshape = TimeDistributed(Reshape(\n                    target_shape=[50]\n                ))(bottom_conv3)\n    # concat output from both pipelines\n    final_concat = Concatenate()([top_dropout, bottom_reshape])\n\n    target_model = TimeDistributed(Dense(output_neurons,name=\"dense\"))(final_concat)\n    model = models.Model(inputs=inputs, outputs = target_model )\n\n    model.compile(loss=\"mean_squared_error\",sample_weight_mode=\"temporal\",optimizer=\"rmsprop\")\n\n    return model,target_model\n\n\n\n\ndef define_model(time_steps,\n                    hidden_neurons = 50,\n                    batch_size=None,\n                    feature_dim =20,\n                    stateful=False):\n    output_neurons = 1\n\n    inputs = Input(batch_shape=(batch_size,time_steps, feature_dim))  #Input(shape=(X_train.shape[1], X_train.shape[2]))\n\n    lstm  = LSTM( hidden_neurons,activation='sigmoid',kernel_initializer='normal', dropout=0.25, return_sequences=True, stateful=stateful, name=\"lstm\")(inputs)\n\n    target_model = TimeDistributed(Dense(output_neurons,name=\"dense\"))(lstm)\n    model = models.Model(inputs=inputs, outputs = target_model )\n\n    model.compile(loss=\"mean_squared_error\",sample_weight_mode=\"temporal\",optimizer=\"rmsprop\")\n\n    return (model,(inputs,lstm,target_model))\n#https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html\nclass statefulModel(object):\n    def __init__(self,model,print_val_every = 500):\n        '''\n        model must be stateful keras model object\n        batch_input_shape must be specified\n        '''\n        bis = model.layers[0].get_config()[\"batch_input_shape\"]\n        print(\"batch_input_shape={}\".format(bis))\n        self.batch_size = bis[0]\n        self.ts         = bis[1]\n        self.Nfeat      = bis[2]\n        self.model      = model\n        self.print_val_every = print_val_every\n\n    def train1epoch(self, X, y, epoch=None):\n        '''\n        devide the training set of time series into batches.\n        '''\n        print( \"  Training..\")\n        batch_index = np.arange(X.shape[0])\n        ## shuffle to create batch containing different time series\n        np.random.shuffle(batch_index)\n        count = 1\n\n        #print(\"batch_index\",batch_index)\n        for ibatch in range(self.batch_size,X.shape[0]+1, self.batch_size):\n\n            print( \"    Batch {:02d}\".format(count))\n            #print( \"ibatch\",ibatch )\n            pick = batch_index[(ibatch-self.batch_size):ibatch]\n            #print(\"pick\", pick  , len(pick))\n            if len(pick) < self.batch_size:\n                continue\n\n            X_batch = X[pick]\n            y_batch = y[pick]\n            #print(\"count\", count)\n            #print(X1_batch.shape)\n            #ibatch = 50\n\n            self.fit_across_time(X_batch,y_batch,epoch,ibatch)\n            count += 1\n            #print(\"count\", count)\n\n    def fit_across_time(self, X, y, epoch=None, ibatch=None):\n        '''\n        training for the given set of time series\n        It always starts at the time point 0 so we need to reset states to zero.\n        '''\n        self.model.reset_states()\n\n        for itime in range(self.ts,X.shape[1]+1,self.ts):\n            ## extract sub time series\n            print(\"itime\",itime)\n\n            Xtime = X[:,itime-self.ts:itime,:]\n            ytime = y[:,itime-self.ts:itime,:]\n\n            #print(X1.shape, X1time.shape ,ytime.shape )\n\n            val = self.model.fit(Xtime,ytime,\n                        nb_epoch=1,\n                        ## no shuffling across rows (i.e. time series)\n                        shuffle=False,\n                        ## use all the samples in one epoch\n                        batch_size=Xtime.shape[0],\n                        verbose= False)\n            if itime % self.print_val_every == 0:\n                print( \"      {start:4d}:{end:4d} loss={val:.3f}\".format(\n                start=itime-self.ts, end=itime, val=val.history[\"loss\"][0]))\n                sys.stdout.flush()\n                ## uncomment below if you do not want to save weights for every epoch every batch and every time\n        if epoch is not None:\n            #path = \"../output\" ../output/\n\n            self.model.save_weights(\n                \"weights_epoch{:03d}_batch{:01d}.hdf5\".format(epoch+1, ibatch))\n\n    def fit(self, X, y, epochs=300):\n\n        past_val_loss = np.Inf\n        history = []\n        for iepoch in range(epochs):\n            self.model.reset_states()\n            print( \"__________________________________\")\n            print( \"Epoch {}\".format(iepoch+1))\n\n            self.train1epoch( X, y, iepoch)\n        return history\n\ndef stateful_prediction( mm,X_test, ntarget=1):\n        #expecting..\n        bis = mm.layers[0].get_config()[\"batch_input_shape\"]\n        batch_size, ts, nfeat = bis\n        assert(X_test.shape[0] % batch_size == 0)\n        assert(X_test.shape[1] % ts == 0)\n\n        y_pred = np.zeros((X_test.shape[0],X_test.shape[1],ntarget))\n        #y_pred[:] = np.NaN\n\n        for ipat in range(0,X_test.shape[0],batch_size):\n            #mm.reset_states()\n            for itime in range(0, X_test.shape[1], ts):\n                X_testi = X_test[ipat:(ipat+batch_size),itime:(itime+ts),:]\n                y_pred[ipat:(ipat+batch_size),itime:(itime+ts),:] = mm.predict(X_testi,batch_size = batch_size)\n                y_pred_temp =  np.squeeze(y_pred[ipat:(ipat+batch_size),itime:(itime+ts),:])\n                y_pred_temp= y_pred_temp.reshape((y_pred_temp.shape[0],1))\n                #print( X_testi[:,:,0].shape, y_pred_temp.shape)\n\n                #sys.exit()\n\n                X_testi[:,:,0] = y_pred_temp#y_pred[ipat:(ipat+batch_size),itime:(itime+ts),:][:,:]\n        return y_pred\ndef unscale(y_arr, scaler, template_df, toint=False):\n    \"\"\"\n    Unscale array y_arr of model predictions, based on a scaler fitted \n    to template_df.\n    \"\"\"\n    tmp = template_df.copy()\n    tmp[y_cols] = pd.DataFrame(y_arr, index=tmp.index)\n    tmp[cols_to_scale] = scaler.inverse_transform(tmp[cols_to_scale])\n    if toint:\n        return tmp[y_cols].astype(int)\n    return tmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aecc2f2cb9a2c9f821487eed52829a4e0bf00cbd"
      },
      "cell_type": "code",
      "source": "X_train, y_train, X_test, scaler, df_test, cols_to_scale, y_cols = generate_data(df_train, df_test )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "953cb9e96f1266df368bd8d6987f809a2f19774d",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "X_trans = X_train.transpose()\ny_train = y_train.transpose()\ny = y_train.values.reshape((500,1825,1))\nX_trans.info()\n\nN= 500\nT= 1825\nD= 20\nx_train = np.zeros((N,T,D))\n\nimport sys\nfor n in range(N+19):\n    for t in range(T):\n        if n<500:\n            x_train[n,t,0] = X_trans.iloc[n][t]\n        else: \n            for d in range(1,D):\n                x_train[:,t,d] = X_trans.iloc[N+d-1][t]"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1e784fbf58b5a82eeca1746919eab64fe148e7c"
      },
      "cell_type": "markdown",
      "source": "hunits = 50\ntime_steps = 365\nepochs = 10\nprint_val_every = 365\nbatch_size = 25\nmodel_stateful, _ = define_model(time_steps,\n                    hidden_neurons = hunits,\n                    batch_size = batch_size,\n                    stateful = True\n                     )\n#model_stateless.summary()\n\nsmodel = statefulModel(model=model_stateful,print_val_every = print_val_every)\n\n#X = [X1_train,X2_train,X3_train,X4_train,X5_train,X6_train]\n#X_val = [X1_val,X2_val,X3_val,X4_val,X5_val,X6_val]\n\nstart = time.time()\nhistory_stateful = smodel.fit(x_train, y, epochs)\n\nend = time.time()\nprint(\"Time Took {:3.2f} min\".format((end - start)/60))\nsys.exit()"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc232b628f07cd35103abe4f5c69280be8909dfa"
      },
      "cell_type": "code",
      "source": "X_test = X_test.transpose()\nN= 500\nT= 90\nD= 20\nx_test_pros = np.zeros((N,T,D))\n\nimport sys\nfor n in range(N+19):\n    for t in range(T):\n        if n<500:\n            x_test_pros[n,t,0] = X_test.iloc[n][t]\n        else: \n            for d in range(1,D):\n                x_test_pros[:,t,d] = X_test.iloc[N+d-1][t]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e72e600f9ed0593dc0d465529385d6fa6a92e9f4"
      },
      "cell_type": "code",
      "source": "def define_model3(time_steps,\n                    hidden_neurons = 50,\n                    batch_size=None,\n                    feature_dim =20,\n                    stateful=False):\n    output_neurons = 1\n\n    inputs = Input( batch_shape=(batch_size,time_steps, feature_dim) )  #Input(shape=(X_train.shape[1], X_train.shape[2]))\n\n    top_lstm  = LSTM( hidden_neurons,activation='sigmoid',kernel_initializer='normal', dropout=0.25, return_sequences=True, stateful=stateful, name=\"lstm\")(inputs)\n    top_dense = TimeDistributed( Dense(hidden_neurons, activation='relu'))(top_lstm)\n    top_dropout = TimeDistributed(Dropout(0.5))(top_dense)\n    # bottom pipeline\n    bottom_dense = TimeDistributed(Dense(hidden_neurons))(inputs)\n    bottom_conv1 = Conv1D(\n                            hidden_neurons,\n                            kernel_size=1,\n                            input_shape=(batch_size, time_steps, feature_dim)#input_shape=(X_train.shape[1], X_train.shape[2])\n                        )(bottom_dense)\n    bottom_conv2 = Conv1D(\n                    50,\n                    kernel_size=50,\n                    padding='same',\n                    activation='relu'\n                )(bottom_conv1)\n    bottom_conv3 = Conv1D(\n                    25,\n                    kernel_size=10,\n                    padding='same',\n                    activation='relu'\n                )(bottom_conv2)\n    bottom_pooling = AvgPool1D(\n                    pool_size=5,\n                    padding='same'\n                )(bottom_conv3)\n    bottom_reshape = TimeDistributed(Reshape(\n                    target_shape=[hidden_neurons]\n                ))(bottom_conv3)\n    # concat output from both pipelines\n    final_concat = Concatenate()([top_dropout, bottom_reshape])\n\n    target_model = TimeDistributed(Dense(output_neurons,name=\"dense\"))(final_concat)\n    model = models.Model(inputs=inputs, outputs = target_model )\n\n    model.compile(loss=\"mean_squared_error\",sample_weight_mode=\"temporal\",optimizer=\"rmsprop\")\n\n    return model,target_model\n#https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92958f7b54af44675ba4198362eb2de6514c8bb8"
      },
      "cell_type": "code",
      "source": "hunits = 20\ntime_steps = 1\nepochs = 20\nprint_val_every = 365\nbatch_size = 25\nmodel_pred1, _ = define_model(time_steps,\n                    hidden_neurons = hunits,\n                    batch_size = batch_size,\n                    stateful = True\n                     )\n\n\nmodel_pred1.load_weights(\"../input/weight-ts6/weights_epoch011_batch500.hdf5\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac7e2df1879986be9dc049c947090ee40ee237e3"
      },
      "cell_type": "code",
      "source": "y_pred_stateful1 = stateful_prediction(mm = model_pred1,X_test = x_test_pros)\ny_pred_stateful122= np.squeeze(y_pred_stateful1 )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6b6ddf508bbb65099e9c08671c1be7de6bcfa8f"
      },
      "cell_type": "code",
      "source": "template_df = df_test\ntemplate_df['is_test'] = np.repeat(True, template_df.shape[0])\nbasic_pred = unscale(y_pred_stateful122.transpose(), scaler, template_df, toint=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69b0d881fba77db7721334916cfe24935a4a3dcd"
      },
      "cell_type": "code",
      "source": "import itertools\ndef sales_by_storeitem(basic_pred):\n    a=[]\n    \n    for  i, s in product(range(1,51), range(1,11) ):\n        #print(\"store, item\",s,i)\n        a.append( basic_pred[f'item_{i}_store_{s}_sales'].values  )\n        #print(len(set(basic_pred.index.values) ))\n        #print((basic_pred.index[basic_pred[f'item_{i}_store_{s}_sales']] ))\n        #ids = (basic_pred.index[basic_pred[f'item_{i}_store_{s}_sales']].tolist() )  #item_9_store_7_sales\n        #print(basic_pred[f'item_{i}_store_{s}_sales'])\n        #print(basic_pred[f'item_{i}_store_{s}_sales'].values)\n        #print(len(set(ids)))\n    a= list(itertools.chain.from_iterable(a))\n\n    return a\n\nsubmission3= sales_by_storeitem( basic_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "731f697ed25bdbc93a78afee444887bb0cec2ec9"
      },
      "cell_type": "code",
      "source": "for idx, row in sample_sub.iterrows():\n    #print(idx,row['store'],row['item'])\n    #sys.exit()\n    row[\"sales\"] =submission3[idx] #y_pred[idx] #np.round(y_pred[idx]).astype(int)\nsample_sub.to_csv(\"submission.csv\", index=False)\nprint( \" submission successful \" )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65ce2b3f4e6036e6b174fed6fd75a21e8b5d3a62"
      },
      "cell_type": "markdown",
      "source": ""
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}